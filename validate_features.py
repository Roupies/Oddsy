import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

class OddsyFeatureValidator:
    """
    Valide que nos features font du SENS dans le contexte football
    """
    
    def __init__(self):
        self.df = None
        self.issues_found = []
        
    def load_data(self, filepath):
        """Load the ML-ready dataset"""
        print("üîç ODDSY FEATURE VALIDATION")
        print("=" * 50)
        
        self.df = pd.read_csv(filepath)
        if 'Date' in self.df.columns:
            self.df['Date'] = pd.to_datetime(self.df['Date'])
            
        print(f"Dataset loaded: {self.df.shape}")
        print(f"Columns: {list(self.df.columns)}")
        return self
    
    def test_basic_data_quality(self):
        """Test 1: V√©rifier la qualit√© de base des donn√©es"""
        print(f"\nüìä TEST 1: QUALIT√â DES DONN√âES")
        print("-" * 30)
        
        issues = []
        
        # Test des valeurs manquantes
        missing = self.df.isnull().sum()
        missing_cols = missing[missing > 0]
        if len(missing_cols) > 0:
            issues.append(f"‚ùå Valeurs manquantes trouv√©es: {dict(missing_cols)}")
        else:
            print("‚úÖ Aucune valeur manquante")
            
        # Test des features normalis√©es (doivent √™tre entre 0 et 1)
        feature_cols = [col for col in self.df.columns if 'normalized' in col or col in ['home_form', 'away_form', 'h2h_score', 'home_advantage']]
        
        for col in feature_cols:
            if col in self.df.columns:
                min_val = self.df[col].min()
                max_val = self.df[col].max()
                if min_val < -0.01 or max_val > 1.01:  # Small tolerance
                    issues.append(f"‚ùå {col}: range [{min_val:.3f}, {max_val:.3f}] - pas entre 0-1")
                else:
                    print(f"‚úÖ {col}: range [{min_val:.3f}, {max_val:.3f}]")
        
        # Test des valeurs infinies
        inf_cols = []
        for col in self.df.select_dtypes(include=[np.number]).columns:
            if np.isinf(self.df[col]).any():
                inf_cols.append(col)
        
        if inf_cols:
            issues.append(f"‚ùå Valeurs infinies dans: {inf_cols}")
        else:
            print("‚úÖ Aucune valeur infinie")
            
        self.issues_found.extend(issues)
        return self
    
    def test_feature_logic(self):
        """Test 2: V√©rifier la logique des features"""
        print(f"\nüß† TEST 2: LOGIQUE DES FEATURES")
        print("-" * 30)
        
        # Test: home_advantage devrait toujours √™tre 1
        if 'home_advantage' in self.df.columns:
            unique_vals = self.df['home_advantage'].unique()
            if len(unique_vals) == 1 and unique_vals[0] == 1:
                print("‚úÖ home_advantage = 1 partout (correct)")
            else:
                self.issues_found.append(f"‚ùå home_advantage a des valeurs bizarres: {unique_vals}")
        
        # Test: form_diff_normalized vs individual forms
        if all(col in self.df.columns for col in ['home_form', 'away_form', 'form_diff_normalized']):
            # Calculer form_diff √† partir des formes individuelles
            calculated_diff = (self.df['home_form'] - self.df['away_form'] + 1) / 2
            actual_diff = self.df['form_diff_normalized']
            
            diff_error = abs(calculated_diff - actual_diff).max()
            if diff_error < 0.01:
                print("‚úÖ form_diff_normalized coh√©rent avec home_form - away_form")
            else:
                self.issues_found.append(f"‚ùå form_diff_normalized incoh√©rent (erreur max: {diff_error:.3f})")
        
        return self
    
    def test_known_matches(self):
        """Test 3: Valider sur des matchs connus o√π on conna√Æt le contexte"""
        print(f"\nüèüÔ∏è TEST 3: VALIDATION SUR MATCHS CONNUS")
        print("-" * 30)
        
        if not all(col in self.df.columns for col in ['Date', 'HomeTeam', 'AwayTeam']):
            print("‚ö†Ô∏è Pas de colonnes Date/Teams - skip test matchs connus")
            return self
        
        # Test cases avec contexte connu
        test_cases = [
            {
                'name': 'Liverpool (fort) vs Norwich (faible) - d√©but 2019-20',
                'home': 'Liverpool',
                'away': 'Norwich',
                'date_range': ('2019-08-01', '2019-09-01'),
                'expected': {
                    'elo_diff_normalized': (0.6, 1.0),  # Liverpool devrait √™tre favori
                    'home_advantage': 1.0,
                    'result_expected': 'H'
                }
            },
            {
                'name': 'Man City (fort) vs √©quipe quelconque',
                'home': 'Man City',
                'away': None,  # N'importe quelle √©quipe
                'date_range': ('2020-01-01', '2020-12-31'),
                'expected': {
                    'elo_diff_normalized': (0.55, 1.0),  # City souvent favori √† domicile
                }
            }
        ]
        
        for test_case in test_cases:
            print(f"\nüîç Test: {test_case['name']}")
            
            # Trouver les matchs correspondants
            mask = (
                (self.df['HomeTeam'] == test_case['home']) &
                (self.df['Date'] >= test_case['date_range'][0]) &
                (self.df['Date'] <= test_case['date_range'][1])
            )
            
            if test_case['away']:
                mask &= (self.df['AwayTeam'] == test_case['away'])
            
            matches = self.df[mask]
            
            if len(matches) == 0:
                print(f"  ‚ö†Ô∏è Aucun match trouv√© pour ce test")
                continue
                
            print(f"  üìä {len(matches)} match(s) trouv√©(s)")
            
            # V√©rifier les expectations
            for feature, expected in test_case['expected'].items():
                if feature == 'result_expected':
                    continue
                    
                # Handle both tuple (min, max) and single value expectations
                if isinstance(expected, tuple):
                    min_exp, max_exp = expected
                else:
                    min_exp = max_exp = expected
                    
                if feature in matches.columns:
                    actual_values = matches[feature]
                    avg_val = actual_values.mean()
                    
                    if min_exp <= avg_val <= max_exp:
                        print(f"  ‚úÖ {feature}: {avg_val:.3f} (dans range attendu [{min_exp:.1f}-{max_exp:.1f}])")
                    else:
                        print(f"  ‚ùå {feature}: {avg_val:.3f} (HORS range attendu [{min_exp:.1f}-{max_exp:.1f}])")
                        self.issues_found.append(f"Match test '{test_case['name']}': {feature} = {avg_val:.3f} (attendu [{min_exp:.1f}-{max_exp:.1f}])")
                        
            # Montrer quelques exemples
            if len(matches) > 0:
                example = matches.iloc[0]
                print(f"  üìã Exemple: {example['HomeTeam']} vs {example['AwayTeam']} ({example['Date'].strftime('%Y-%m-%d') if 'Date' in example else 'date inconnue'})")
                
                feature_cols = ['elo_diff_normalized', 'form_diff_normalized', 'h2h_score']
                for col in feature_cols:
                    if col in example:
                        print(f"       {col}: {example[col]:.3f}")
        
        return self
    
    def test_distribution_sanity(self):
        """Test 4: V√©rifier que les distributions sont sens√©es"""
        print(f"\nüìà TEST 4: DISTRIBUTIONS DES FEATURES")
        print("-" * 30)
        
        feature_cols = [col for col in self.df.columns if any(x in col for x in ['form', 'elo', 'h2h', 'advantage', 'normalized'])]
        
        for col in feature_cols[:5]:  # Top 5 features
            if col in self.df.columns:
                data = self.df[col]
                
                print(f"\nüìä {col}:")
                print(f"   Mean: {data.mean():.3f}")
                print(f"   Std:  {data.std():.3f}")
                print(f"   Min:  {data.min():.3f}")
                print(f"   Max:  {data.max():.3f}")
                
                # Test si trop de valeurs identiques (suspect)
                value_counts = data.value_counts()
                most_common_pct = value_counts.iloc[0] / len(data) * 100
                
                if most_common_pct > 50:
                    self.issues_found.append(f"‚ùå {col}: {most_common_pct:.1f}% des valeurs identiques (valeur: {value_counts.index[0]})")
                    print(f"   ‚ö†Ô∏è {most_common_pct:.1f}% des valeurs = {value_counts.index[0]}")
                else:
                    print(f"   ‚úÖ Distribution vari√©e (valeur max: {most_common_pct:.1f}%)")
        
        return self
    
    def test_target_distribution(self):
        """Test 5: V√©rifier la distribution des r√©sultats"""
        print(f"\nüéØ TEST 5: DISTRIBUTION DES R√âSULTATS")
        print("-" * 30)
        
        if 'FullTimeResult' in self.df.columns:
            target_counts = self.df['FullTimeResult'].value_counts()
            total = len(self.df)
            
            print("Distribution des r√©sultats:")
            for result in ['H', 'D', 'A']:
                count = target_counts.get(result, 0)
                pct = count / total * 100
                print(f"  {result}: {count} matchs ({pct:.1f}%)")
            
            # V√©rifier si trop d√©s√©quilibr√©
            min_pct = min([target_counts.get(r, 0) / total * 100 for r in ['H', 'D', 'A']])
            max_pct = max([target_counts.get(r, 0) / total * 100 for r in ['H', 'D', 'A']])
            
            if max_pct - min_pct > 20:  # Plus de 20% d'√©cart
                self.issues_found.append(f"‚ùå Dataset d√©s√©quilibr√©: √©cart {max_pct - min_pct:.1f}% entre min et max")
            else:
                print(f"‚úÖ Distribution √©quilibr√©e (√©cart: {max_pct - min_pct:.1f}%)")
        
        return self
    
    def generate_report(self):
        """G√©n√©rer le rapport final"""
        print(f"\nüìã RAPPORT FINAL DE VALIDATION")
        print("=" * 50)
        
        if len(self.issues_found) == 0:
            print("üéâ TOUTES LES VALIDATIONS PASS√âES !")
            print("‚úÖ Dataset pr√™t pour le Machine Learning")
        else:
            print(f"‚ö†Ô∏è {len(self.issues_found)} PROBL√àME(S) D√âTECT√â(S):")
            for i, issue in enumerate(self.issues_found, 1):
                print(f"{i:2d}. {issue}")
            
            print(f"\nüîß Action requise: Corriger ces probl√®mes avant ML")
        
        # Stats g√©n√©rales
        print(f"\nüìä STATISTIQUES G√âN√âRALES:")
        print(f"   ‚Ä¢ Dataset: {self.df.shape[0]} matchs, {self.df.shape[1]} colonnes")
        
        if 'Date' in self.df.columns:
            print(f"   ‚Ä¢ P√©riode: {self.df['Date'].min()} √† {self.df['Date'].max()}")
        
        feature_cols = [col for col in self.df.columns if any(x in col for x in ['form', 'elo', 'h2h', 'advantage', 'normalized'])]
        print(f"   ‚Ä¢ Features ML: {len(feature_cols)} features")
        
        return len(self.issues_found) == 0

# Ex√©cution
if __name__ == "__main__":
    # Tester sur le dataset enhanced (le plus r√©cent)
    validator = OddsyFeatureValidator()
    
    # Essayer diff√©rents fichiers selon ce qui existe
    possible_files = [
        '/Users/maxime/Desktop/Oddsy/data/processed/premier_league_ml_ready.csv',
        '/Users/maxime/Desktop/Oddsy/data/processed/premier_league_2019_2024_final.csv',
        '/Users/maxime/Desktop/Oddsy/data/processed/premier_league_2019_2024_enhanced.csv'
    ]
    
    import os
    file_found = None
    for filepath in possible_files:
        if os.path.exists(filepath):
            file_found = filepath
            break
    
    if file_found:
        print(f"üéØ Testing file: {file_found}")
        
        validator.load_data(file_found)
        validator.test_basic_data_quality()
        validator.test_feature_logic()
        validator.test_known_matches()
        validator.test_distribution_sanity()
        validator.test_target_distribution()
        
        is_valid = validator.generate_report()
        
        if is_valid:
            print(f"\nüöÄ PR√äT POUR LA PHASE 5 : MACHINE LEARNING!")
        else:
            print(f"\nüîß CORRIGER LES PROBL√àMES AVANT ML")
            
    else:
        print("‚ùå Aucun fichier de donn√©es trouv√©!")
        print("Fichiers cherch√©s:")
        for f in possible_files:
            print(f"  - {f}")
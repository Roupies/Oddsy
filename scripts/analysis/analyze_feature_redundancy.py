#!/usr/bin/env python3
"""
analyze_feature_redundancy.py

ANALYSE COMPL√àTE DE REDONDANCE DES 27 FEATURES v2.2
Bas√© sur l'analyse critique de Claude Opus

OBJECTIF:
- Identifier les features redondantes parmi les 27 actuelles
- Passer de 27 ‚Üí 10-12 features optimales
- Pr√©parer base saine pour features contextuelles

USAGE:
python scripts/analysis/analyze_feature_redundancy.py
"""

import sys
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr, pearsonr
from sklearn.feature_selection import (
    VarianceThreshold, 
    mutual_info_classif,
    SelectKBest,
    RFE,
    RFECV
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from utils import setup_logging

class FeatureRedundancyAnalyzer:
    """
    Analyseur de redondance des features pour Oddsy v2.2
    """
    
    def __init__(self, correlation_threshold=0.80, variance_threshold=0.01):
        self.corr_threshold = correlation_threshold
        self.var_threshold = variance_threshold
        self.results = {}
        self.logger = setup_logging()
        
    def load_data(self, filepath='data/processed/v13_xg_safe_features.csv'):
        """
        Charger les donn√©es v2.2 avec 27 features
        """
        self.logger.info("üìä CHARGEMENT DES DONN√âES v2.2")
        self.logger.info("="*60)
        
        df = pd.read_csv(filepath, parse_dates=['Date'])
        self.logger.info(f"‚úÖ Donn√©es charg√©es: {len(df)} matches")
        
        # Liste exacte des 27 features v2.2 (bas√©e sur v13_xg_safe_features.csv)
        self.feature_names = [
            col for col in df.columns 
            if col not in ['Date', 'Season', 'HomeTeam', 'AwayTeam', 'FullTimeResult']
        ]
        
        self.logger.info(f"üìà Features d√©tect√©es: {len(self.feature_names)}")
        for i, feat in enumerate(self.feature_names, 1):
            self.logger.info(f"  {i:2d}. {feat}")
        
        # Pr√©parer X et y avec notre split temporel
        train_end = pd.to_datetime('2024-05-19')  # Fin training v2.1
        test_start = pd.to_datetime('2024-08-16')  # D√©but test 2024-2025
        
        # Filtrer donn√©es non-manquantes
        valid_data = df.dropna(subset=self.feature_names)
        self.logger.info(f"üìä Matches avec toutes features: {len(valid_data)}")
        
        # Split temporel
        train_data = valid_data[valid_data['Date'] <= train_end].copy()
        test_data = valid_data[(valid_data['Date'] >= test_start)].copy()
        
        self.logger.info(f"üìä Train: {len(train_data)} matches (jusqu'√† {train_end.strftime('%Y-%m-%d')})")
        self.logger.info(f"üìä Test: {len(test_data)} matches (depuis {test_start.strftime('%Y-%m-%d')})")
        
        # Pr√©parer matrices
        self.X_train = train_data[self.feature_names].values
        self.X_test = test_data[self.feature_names].values
        
        # Target encoding: H->0, D->1, A->2
        self.y_train = train_data['FullTimeResult'].map({'H': 0, 'D': 1, 'A': 2}).values
        self.y_test = test_data['FullTimeResult'].map({'H': 0, 'D': 1, 'A': 2}).values
        
        # Handle any remaining NaN with 0.5 (neutral value)
        self.X_train = np.nan_to_num(self.X_train, nan=0.5)
        self.X_test = np.nan_to_num(self.X_test, nan=0.5)
        
        self.logger.info(f"‚úÖ Donn√©es pr√™tes: X_train {self.X_train.shape}, X_test {self.X_test.shape}")
        
        return self.X_train, self.y_train, self.X_test, self.y_test
    
    def analyze_correlations(self):
        """
        Analyser les corr√©lations entre features - Focus xG redondance
        """
        self.logger.info("\nüîç ANALYSE DES CORR√âLATIONS (Focus xG)")
        self.logger.info("="*60)
        
        # DataFrame pour corr√©lations
        df_train = pd.DataFrame(self.X_train, columns=self.feature_names)
        
        # Matrices de corr√©lation (Pearson ET Spearman)
        corr_pearson = df_train.corr(method='pearson')
        corr_spearman = df_train.corr(method='spearman')
        
        # Identifier paires fortement corr√©l√©es
        high_corr_pairs = []
        for i in range(len(self.feature_names)):
            for j in range(i+1, len(self.feature_names)):
                pearson_val = abs(corr_pearson.iloc[i, j])
                spearman_val = abs(corr_spearman.iloc[i, j])
                
                # Utiliser la plus forte corr√©lation
                max_corr = max(pearson_val, spearman_val)
                
                if max_corr > self.corr_threshold:
                    high_corr_pairs.append({
                        'feature1': self.feature_names[i],
                        'feature2': self.feature_names[j],
                        'pearson': corr_pearson.iloc[i, j],
                        'spearman': corr_spearman.iloc[i, j],
                        'max_abs_corr': max_corr
                    })
        
        # Trier par corr√©lation maximale
        high_corr_pairs = sorted(high_corr_pairs, 
                                key=lambda x: x['max_abs_corr'], 
                                reverse=True)
        
        self.logger.info(f"\nüî¥ {len(high_corr_pairs)} paires avec corr√©lation > {self.corr_threshold}:")
        self.logger.info("-"*80)
        for i, pair in enumerate(high_corr_pairs, 1):
            self.logger.info(f"{i:2}. {pair['feature1']:25} <-> {pair['feature2']:25}")
            self.logger.info(f"    Pearson: {pair['pearson']:6.3f} | Spearman: {pair['spearman']:6.3f}")
        
        # Focus sur les groupes redondants xG
        self._analyze_xg_redundancy(df_train)
        
        # Identifier features √† supprimer
        features_to_remove = self._identify_redundant_features(high_corr_pairs, df_train)
        
        self.results['correlation_analysis'] = {
            'high_corr_pairs': high_corr_pairs,
            'features_to_remove': list(features_to_remove),
            'pearson_matrix': corr_pearson,
            'spearman_matrix': corr_spearman
        }
        
        self.logger.info(f"\nüóëÔ∏è Features √† supprimer ({len(features_to_remove)}):")
        for feat in features_to_remove:
            self.logger.info(f"  - {feat}")
        
        return features_to_remove
    
    def _analyze_xg_redundancy(self, df_train):
        """
        Analyse sp√©cifique de la redondance xG
        """
        self.logger.info("\n‚öΩ ANALYSE SP√âCIFIQUE xG REDONDANCE")
        self.logger.info("-"*50)
        
        # Groupes xG identifi√©s
        xg_groups = {
            'Home xG Rolling': [f for f in self.feature_names if 'home_xg_roll' in f],
            'Away xG Rolling': [f for f in self.feature_names if 'away_xg_roll' in f],
            'Home Goals Sum': [f for f in self.feature_names if 'home_goals_sum' in f],
            'Away Goals Sum': [f for f in self.feature_names if 'away_goals_sum' in f],
            'Home xG Sum': [f for f in self.feature_names if 'home_xg_sum' in f],
            'Away xG Sum': [f for f in self.feature_names if 'away_xg_sum' in f],
            'xG Efficiency': [f for f in self.feature_names if 'xg_eff' in f],
            'xG Differences': [f for f in self.feature_names if 'xg_roll' in f and 'diff' in f]
        }
        
        # Analyser chaque groupe
        for group_name, features in xg_groups.items():
            if len(features) > 1:
                group_corr = df_train[features].corr(method='pearson')
                # Moyenne corr√©lations hors diagonale
                n = len(features)
                avg_corr = (group_corr.abs().sum().sum() - n) / (n * (n - 1))
                
                self.logger.info(f"\n{group_name}:")
                self.logger.info(f"  Features ({len(features)}): {features}")
                self.logger.info(f"  Corr√©lation moyenne: {avg_corr:.3f}")
                
                if avg_corr > 0.85:
                    self.logger.info("  üî¥ TR√àS REDONDANT - Garder 1 feature max")
                elif avg_corr > 0.70:
                    self.logger.info("  üü° REDONDANT - Garder 1-2 features max")
                else:
                    self.logger.info("  ‚úÖ Acceptable")
    
    def _identify_redundant_features(self, high_corr_pairs, df_train):
        """
        Strat√©gie intelligente pour supprimer redondances
        """
        # Calculer importance de chaque feature (MI avec target)
        mi_scores = mutual_info_classif(self.X_train, self.y_train, random_state=42)
        importance_scores = dict(zip(self.feature_names, mi_scores))
        
        features_to_remove = set()
        
        # Strat√©gie: Dans chaque paire corr√©l√©e, supprimer la moins informative
        for pair in high_corr_pairs:
            f1, f2 = pair['feature1'], pair['feature2']
            
            # Si une des deux est d√©j√† marqu√©e pour suppression, passer
            if f1 in features_to_remove or f2 in features_to_remove:
                continue
            
            # Comparer importance
            if importance_scores[f1] < importance_scores[f2]:
                features_to_remove.add(f1)
            else:
                features_to_remove.add(f2)
        
        # R√®gles sp√©ciales pour xG (bas√© sur analyse Claude Opus)
        xg_special_rules = self._apply_xg_special_rules()
        features_to_remove.update(xg_special_rules)
        
        return features_to_remove
    
    def _apply_xg_special_rules(self):
        """
        R√®gles sp√©ciales pour nettoyer les features xG
        """
        to_remove = set()
        
        # R√®gle 1: Si on a roll_5 et roll_10, garder seulement roll_5
        if 'home_xg_roll_5' in self.feature_names and 'home_xg_roll_10' in self.feature_names:
            to_remove.add('home_xg_roll_10')
        if 'away_xg_roll_5' in self.feature_names and 'away_xg_roll_10' in self.feature_names:
            to_remove.add('away_xg_roll_10')
        
        # R√®gle 2: Si on a sum_5 et sum_10, garder seulement sum_5
        if 'home_xg_sum_5' in self.feature_names and 'home_xg_sum_10' in self.feature_names:
            to_remove.add('home_xg_sum_10')
        if 'away_xg_sum_5' in self.feature_names and 'away_xg_sum_10' in self.feature_names:
            to_remove.add('away_xg_sum_10')
        
        # R√®gle 3: Si on a diff et diff_normalized, garder seulement normalized
        if 'xg_roll_5_diff' in self.feature_names and 'xg_roll_5_diff_normalized' in self.feature_names:
            to_remove.add('xg_roll_5_diff')
        if 'xg_roll_10_diff' in self.feature_names and 'xg_roll_10_diff_normalized' in self.feature_names:
            to_remove.add('xg_roll_10_diff')
        
        return to_remove
    
    def analyze_mutual_information(self):
        """
        Information mutuelle avec la target (H/D/A)
        """
        self.logger.info("\nüéØ ANALYSE D'INFORMATION MUTUELLE")
        self.logger.info("="*60)
        
        # Calculer MI scores
        mi_scores = mutual_info_classif(self.X_train, self.y_train, random_state=42)
        
        # DataFrame pour analyse
        mi_df = pd.DataFrame({
            'feature': self.feature_names,
            'mi_score': mi_scores
        }).sort_values('mi_score', ascending=False)
        
        self.logger.info(f"\nüèÜ TOP 15 FEATURES par Information Mutuelle:")
        self.logger.info("-"*70)
        for i, (_, row) in enumerate(mi_df.head(15).iterrows(), 1):
            self.logger.info(f"{i:2}. {row['feature']:35} : {row['mi_score']:.6f}")
        
        # Identifier les features avec MI tr√®s faible
        low_mi_threshold = 0.001
        low_mi_features = mi_df[mi_df['mi_score'] < low_mi_threshold]['feature'].tolist()
        
        if low_mi_features:
            self.logger.info(f"\n‚ö†Ô∏è Features avec MI < {low_mi_threshold}:")
            for feat in low_mi_features:
                mi_val = mi_df[mi_df['feature'] == feat]['mi_score'].values[0]
                self.logger.info(f"  - {feat}: {mi_val:.6f}")
        
        self.results['mutual_information'] = {
            'mi_df': mi_df,
            'low_mi_features': low_mi_features
        }
        
        return mi_df
    
    def analyze_variance(self):
        """
        Identifier features avec peu de variance (peu informatives)
        """
        self.logger.info("\nüìä ANALYSE DE VARIANCE")
        self.logger.info("="*60)
        
        # Standardiser pour comparer les variances
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.X_train)
        
        # Calculer variances
        variances = np.var(X_scaled, axis=0)
        
        # DataFrame pour analyse
        var_df = pd.DataFrame({
            'feature': self.feature_names,
            'variance': variances
        }).sort_values('variance')
        
        # Features avec faible variance
        low_var_features = var_df[var_df['variance'] < self.var_threshold]['feature'].tolist()
        
        self.logger.info(f"\nüìä VARIANCE DES FEATURES (standardis√©es):")
        self.logger.info("-"*60)
        for _, row in var_df.iterrows():
            status = "üî¥" if row['variance'] < self.var_threshold else "‚úÖ"
            self.logger.info(f"{status} {row['feature']:35} : {row['variance']:.6f}")
        
        if low_var_features:
            self.logger.info(f"\nüî¥ {len(low_var_features)} features avec variance < {self.var_threshold}")
        
        self.results['variance_analysis'] = {
            'variance_df': var_df,
            'low_variance_features': low_var_features
        }
        
        return low_var_features
    
    def select_optimal_features(self, target_n_features=12):
        """
        S√©lection finale des features optimales
        """
        self.logger.info(f"\n‚ú® S√âLECTION DE {target_n_features} FEATURES OPTIMALES")
        self.logger.info("="*60)
        
        # √âtape 1: √âliminer redondances et faible variance
        redundant = self.results['correlation_analysis']['features_to_remove']
        low_var = self.results['variance_analysis']['low_variance_features']
        low_mi = self.results['mutual_information']['low_mi_features']
        
        to_exclude = set(redundant) | set(low_var) | set(low_mi)
        features_stage1 = [f for f in self.feature_names if f not in to_exclude]
        
        self.logger.info(f"\nüìå Apr√®s nettoyage automatique: {len(features_stage1)} features")
        self.logger.info(f"   Supprim√©es: {len(to_exclude)} ({len(redundant)} redondantes + {len(low_var)} faible variance + {len(low_mi)} faible MI)")
        
        # √âtape 2: Forcer inclusion des features critiques v2.1
        critical_features = [
            'elo_diff_normalized',
            'form_diff_normalized', 
            'h2h_score',
            'market_entropy_norm',
            'matchday_normalized'
        ]
        
        # Ajouter les features critiques si elles existent et ne sont pas d√©j√† incluses
        for feat in critical_features:
            if feat in self.feature_names and feat not in features_stage1:
                features_stage1.append(feat)
                self.logger.info(f"   ‚ûï Ajout forc√© feature critique: {feat}")
        
        # √âtape 3: Si on a encore trop de features, utiliser RFE
        if len(features_stage1) > target_n_features:
            self.logger.info(f"\nüéØ RFE pour r√©duire {len(features_stage1)} ‚Üí {target_n_features}")
            
            X_filtered = self.X_train[:, [self.feature_names.index(f) for f in features_stage1]]
            
            rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
            
            # RFE standard pour le nombre exact demand√©
            rfe = RFE(rf, n_features_to_select=target_n_features, step=1)
            rfe.fit(X_filtered, self.y_train)
            
            selected_features = [features_stage1[i] for i, selected in 
                               enumerate(rfe.support_) if selected]
            
            # Forcer inclusion des features critiques m√™me apr√®s RFE
            for feat in critical_features:
                if feat in self.feature_names and feat not in selected_features:
                    # Remplacer la feature la moins importante
                    if len(selected_features) >= target_n_features:
                        # Enlever la derni√®re feature selon ranking RFE
                        worst_idx = np.argmax(rfe.ranking_)
                        worst_feature = features_stage1[worst_idx]
                        if worst_feature in selected_features:
                            selected_features.remove(worst_feature)
                    selected_features.append(feat)
                    self.logger.info(f"   ‚ûï Inclusion forc√©e post-RFE: {feat}")
        else:
            selected_features = features_stage1
        
        # Affichage final
        self.logger.info(f"\nüèÜ FEATURES FINALES S√âLECTIONN√âES ({len(selected_features)}):")
        self.logger.info("-"*70)
        
        # R√©cup√©rer MI scores pour affichage
        mi_df = self.results['mutual_information']['mi_df']
        
        for i, feat in enumerate(selected_features, 1):
            mi_score = mi_df[mi_df['feature'] == feat]['mi_score'].values[0]
            is_critical = "‚≠ê" if feat in critical_features else "  "
            self.logger.info(f"{i:2}. {feat:35} (MI: {mi_score:.6f}) {is_critical}")
        
        self.results['selected_features'] = selected_features
        return selected_features
    
    def evaluate_selection(self, selected_features):
        """
        √âvaluer la performance avec features s√©lectionn√©es vs compl√®tes
        """
        self.logger.info(f"\nüìä √âVALUATION DE LA S√âLECTION")
        self.logger.info("="*60)
        
        # Pr√©parer les donn√©es
        selected_indices = [self.feature_names.index(f) for f in selected_features]
        X_train_selected = self.X_train[:, selected_indices]
        X_test_selected = self.X_test[:, selected_indices]
        
        # Mod√®le simple pour comparaison
        rf = RandomForestClassifier(
            n_estimators=200, 
            random_state=42, 
            n_jobs=-1,
            class_weight='balanced'
        )
        
        # √âvaluation avec toutes les features
        rf.fit(self.X_train, self.y_train)
        score_all = rf.score(self.X_test, self.y_test)
        
        # √âvaluation avec features s√©lectionn√©es
        rf.fit(X_train_selected, self.y_train)
        score_selected = rf.score(X_test_selected, self.y_test)
        
        # Feature importance des features s√©lectionn√©es
        importance = dict(zip(selected_features, rf.feature_importances_))
        sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)
        
        self.logger.info(f"\nüìà COMPARAISON DE PERFORMANCE:")
        self.logger.info(f"  {len(self.feature_names)} features compl√®tes: {score_all:.4f} ({score_all*100:.2f}%)")
        self.logger.info(f"  {len(selected_features)} features s√©lectionn√©es: {score_selected:.4f} ({score_selected*100:.2f}%)")
        self.logger.info(f"  Diff√©rence: {(score_selected - score_all)*100:+.2f}pp")
        
        # √âvaluation du succ√®s
        if score_selected >= score_all - 0.005:  # Tol√©rance 0.5pp
            self.logger.info("\n‚úÖ S√âLECTION R√âUSSIE: Performance maintenue avec moins de features!")
            success = True
        else:
            self.logger.info("\n‚ö†Ô∏è Perte de performance - Revoir la s√©lection")
            success = False
        
        self.logger.info(f"\nüèÜ TOP 10 IMPORTANCE (features s√©lectionn√©es):")
        for i, (feat, imp) in enumerate(sorted_importance[:10], 1):
            self.logger.info(f"  {i:2}. {feat:35} : {imp:.4f}")
        
        self.results['evaluation'] = {
            'score_all_features': score_all,
            'score_selected_features': score_selected,
            'performance_diff': score_selected - score_all,
            'success': success,
            'feature_importance': sorted_importance
        }
        
        return success, score_selected, score_all
    
    def generate_comprehensive_report(self):
        """
        Rapport complet et actionnable
        """
        self.logger.info(f"\nüìù RAPPORT FINAL - ANALYSE REDONDANCE v2.2")
        self.logger.info("="*80)
        
        # R√©sum√© des analyses
        redundant = self.results['correlation_analysis']['features_to_remove']
        low_var = self.results['variance_analysis']['low_variance_features']
        low_mi = self.results['mutual_information']['low_mi_features']
        selected = self.results['selected_features']
        
        self.logger.info(f"\nüìä R√âSUM√â QUANTITATIF:")
        self.logger.info(f"  Features originales: {len(self.feature_names)}")
        self.logger.info(f"  Features redondantes: {len(redundant)}")
        self.logger.info(f"  Features faible variance: {len(low_var)}")
        self.logger.info(f"  Features faible MI: {len(low_mi)}")
        self.logger.info(f"  Features finales: {len(selected)}")
        self.logger.info(f"  R√©duction: {((len(self.feature_names) - len(selected)) / len(self.feature_names) * 100):.1f}%")
        
        # Performance
        if 'evaluation' in self.results:
            eval_results = self.results['evaluation']
            self.logger.info(f"\nüìà IMPACT PERFORMANCE:")
            self.logger.info(f"  Avant: {eval_results['score_all_features']:.4f} ({eval_results['score_all_features']*100:.2f}%)")
            self.logger.info(f"  Apr√®s: {eval_results['score_selected_features']:.4f} ({eval_results['score_selected_features']*100:.2f}%)")
            self.logger.info(f"  Diff√©rence: {eval_results['performance_diff']*100:+.2f}pp")
            status = "‚úÖ SUCC√àS" if eval_results['success'] else "‚ùå √âCHEC"
            self.logger.info(f"  Statut: {status}")
        
        # Recommandations concr√®tes
        self.logger.info(f"\nüéØ RECOMMANDATIONS IMM√âDIATES:")
        self.logger.info(f"  1. ‚úÖ Utiliser les {len(selected)} features optimales identifi√©es")
        self.logger.info(f"  2. üóëÔ∏è Supprimer les {len(redundant)} features redondantes xG")
        self.logger.info(f"  3. üî¨ Tester XGBoost/LightGBM sur features nettoy√©es")
        self.logger.info(f"  4. üèóÔ∏è Impl√©menter mod√®le cascade sp√©cialis√© Draws")
        self.logger.info(f"  5. üåü Ajouter features contextuelles (m√©t√©o, arbitres, fatigue)")
        
        # Prochaines √©tapes
        self.logger.info(f"\nüöÄ PROCHAINES √âTAPES:")
        self.logger.info(f"  Phase 1: Entra√Æner v2.3 avec {len(selected)} features nettoy√©es")
        self.logger.info(f"  Phase 2: D√©velopper features contextuelles innovantes")
        self.logger.info(f"  Phase 3: Architecture cascade H/D/A sp√©cialis√©e")
        self.logger.info(f"  Objectif: D√©passer 55% (actuellement ~54.2%)")
        
        # Sauvegarder rapport d√©taill√©
        timestamp = datetime.now().strftime('%Y_%m_%d_%H%M%S')
        
        report = {
            'timestamp': timestamp,
            'analysis_summary': {
                'n_original_features': len(self.feature_names),
                'n_redundant_features': len(redundant),
                'n_low_variance_features': len(low_var),
                'n_low_mi_features': len(low_mi),
                'n_selected_features': len(selected),
                'reduction_percentage': (len(self.feature_names) - len(selected)) / len(self.feature_names) * 100
            },
            'feature_lists': {
                'original_features': self.feature_names,
                'redundant_features': list(redundant),
                'low_variance_features': low_var,
                'low_mi_features': low_mi,
                'selected_features': selected
            },
            'performance_impact': self.results.get('evaluation', {}),
            'high_correlation_pairs': self.results['correlation_analysis']['high_corr_pairs'][:20],
            'recommendations': {
                'immediate': [
                    f"Use {len(selected)} optimal features instead of {len(self.feature_names)}",
                    "Remove redundant xG features",
                    "Test XGBoost/LightGBM on cleaned features",
                    "Implement cascade model for Draws",
                    "Add contextual features (weather, referees, fatigue)"
                ],
                'next_phases': [
                    "Train v2.3 with cleaned features",
                    "Develop innovative contextual features", 
                    "Implement H/D/A specialized cascade architecture"
                ]
            }
        }
        
        output_file = f'reports/feature_redundancy_analysis_{timestamp}.json'
        os.makedirs('reports', exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.logger.info(f"\n‚úÖ RAPPORT D√âTAILL√â SAUVEGARD√â: {output_file}")
        
        return report

def main():
    """
    Script principal d'analyse de redondance
    """
    print("üöÄ ANALYSE DE REDONDANCE DES FEATURES - ODDSY v2.2")
    print("="*80)
    print("Objectif: 27 features ‚Üí 10-12 features optimales")
    print("Base: Analyse critique Claude Opus")
    print("="*80)
    
    # Initialiser analyseur
    analyzer = FeatureRedundancyAnalyzer(
        correlation_threshold=0.80,  # Plus permissif pour premi√®re analyse
        variance_threshold=0.01      # Seuil variance standardis√©e
    )
    
    try:
        # 1. Charger donn√©es
        X_train, y_train, X_test, y_test = analyzer.load_data()
        
        # 2. Analyses principales
        redundant_features = analyzer.analyze_correlations()
        low_var_features = analyzer.analyze_variance() 
        mi_df = analyzer.analyze_mutual_information()
        
        # 3. S√©lection optimale
        selected_features = analyzer.select_optimal_features(target_n_features=12)
        
        # 4. √âvaluation
        success, score_selected, score_all = analyzer.evaluate_selection(selected_features)
        
        # 5. Rapport final
        report = analyzer.generate_comprehensive_report()
        
        # 6. Status final
        print("\n" + "="*80)
        print("üéØ ANALYSE TERMIN√âE!")
        print(f"‚úÖ Features optimales: {len(selected_features)}/{len(analyzer.feature_names)}")
        print(f"üìä Performance maintenue: {success}")
        print(f"üìà Diff√©rence: {(score_selected - score_all)*100:+.2f}pp")
        print("üöÄ Pr√™t pour v2.3 avec features nettoy√©es!")
        print("="*80)
        
        return 0
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
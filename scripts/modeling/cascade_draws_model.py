#!/usr/bin/env python3
"""
cascade_draws_model.py

MOD√àLE CASCADE SP√âCIALIS√â POUR LES DRAWS
Impl√©mentation de l'architecture H/D/A cascade

PROBL√àME IDENTIFI√â:
- Draw F1-score: 0.02-0.04 (quasi inexistant)
- Mod√®le global √©choue sur classe minoritaire (23% Draws)
- Besoin d'architecture sp√©cialis√©e

ARCHITECTURE CASCADE:
1. Classificateur binaire: Match nul ? (D vs H+A)
2. Si non-match nul: Classificateur Home vs Away
3. Optimisation s√©par√©e pour chaque √©tage

OBJECTIF:
- Am√©liorer drastiquement pr√©diction Draws
- Maintenir performance globale H/A  
- Passer F1-Draw de 0.04 ‚Üí 0.20+
"""

import sys
import os
import json
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# ML imports
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, StratifiedKFold
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import (
    accuracy_score, 
    classification_report, 
    log_loss, 
    confusion_matrix,
    precision_recall_fscore_support,
    roc_auc_score
)
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
import xgboost as xgb

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from utils import setup_logging

class CascadeDrawsModel:
    """
    Architecture cascade sp√©cialis√©e pour am√©liorer pr√©diction des Draws
    """
    
    def __init__(self, features_list=None):
        self.logger = setup_logging()
        
        # Features optimales (post analyse redondance)
        self.features = features_list or [
            'form_diff_normalized',      # ‚≠ê v2.1 core
            'elo_diff_normalized',       # ‚≠ê v2.1 core  
            'h2h_score',                 # ‚≠ê v2.1 core
            'matchday_normalized',       # ‚≠ê v2.1 core
            'shots_diff_normalized',
            'corners_diff_normalized',
            'market_entropy_norm',       # ‚≠ê v2.1 core
            'home_xg_eff_10',
            'away_goals_sum_5',
            'away_xg_eff_10'
        ]
        
        self.draw_classifier = None      # √âtage 1: Draw vs Non-Draw
        self.home_away_classifier = None # √âtage 2: Home vs Away
        self.scaler = StandardScaler()
        
        self.results = {}
        
    def load_data(self, filepath='data/processed/v13_xg_safe_features.csv'):
        """
        Charger et pr√©parer donn√©es pour cascade
        """
        self.logger.info("üìä CHARGEMENT DONN√âES POUR CASCADE DRAWS")
        self.logger.info("="*70)
        
        df = pd.read_csv(filepath, parse_dates=['Date'])
        
        # Filtrer donn√©es compl√®tes
        valid_data = df.dropna(subset=self.features)
        self.logger.info(f"‚úÖ Donn√©es: {len(valid_data)} matches avec {len(self.features)} features")
        
        # Split temporel
        train_end = pd.to_datetime('2024-05-19')
        test_start = pd.to_datetime('2024-08-16')
        
        train_data = valid_data[valid_data['Date'] <= train_end].copy()
        test_data = valid_data[valid_data['Date'] >= test_start].copy()
        
        self.logger.info(f"üìä Train: {len(train_data)}, Test: {len(test_data)}")
        
        # Matrices features
        self.X_train = train_data[self.features].values
        self.X_test = test_data[self.features].values
        
        # Targets multiples pour cascade
        train_results = train_data['FullTimeResult'].values
        test_results = test_data['FullTimeResult'].values
        
        # 1. Target Draw vs Non-Draw (√©tage 1)\n        self.y_train_draw = (train_results == 'D').astype(int)  # 1=Draw, 0=Non-Draw
        self.y_test_draw = (test_results == 'D').astype(int)
        
        # 2. Target Home vs Away pour non-draws (√©tage 2)
        non_draw_train_mask = train_results != 'D'
        non_draw_test_mask = test_results != 'D'
        
        self.X_train_ha = self.X_train[non_draw_train_mask]
        self.X_test_ha = self.X_test[non_draw_test_mask]
        
        # Pour Home/Away: H=1, A=0
        self.y_train_ha = (train_results[non_draw_train_mask] == 'H').astype(int)
        self.y_test_ha = (test_results[non_draw_test_mask] == 'H').astype(int)
        
        # Target globale pour √©valuation finale (H=0, D=1, A=2)
        self.y_train_global = pd.Series(train_results).map({'H': 0, 'D': 1, 'A': 2}).values
        self.y_test_global = pd.Series(test_results).map({'H': 0, 'D': 1, 'A': 2}).values
        
        # Standardisation
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        self.X_train_ha_scaled = self.scaler.transform(self.X_train_ha)
        self.X_test_ha_scaled = self.scaler.transform(self.X_test_ha)
        
        # Analyse distribution
        draw_pct = np.mean(self.y_train_draw) * 100
        home_pct_non_draw = np.mean(self.y_train_ha) * 100
        
        self.logger.info(f"üìä Distribution training:")
        self.logger.info(f"  Draws: {np.sum(self.y_train_draw)} ({draw_pct:.1f}%)")
        self.logger.info(f"  Non-Draws: {len(self.y_train_draw) - np.sum(self.y_train_draw)} ({100-draw_pct:.1f}%)")
        self.logger.info(f"  Home wins (dans non-draws): {np.sum(self.y_train_ha)} ({home_pct_non_draw:.1f}%)")
        
        return self.X_train, self.y_train_global, self.X_test, self.y_test_global
    
    def train_draw_classifier(self):
        """
        √âtage 1: Classificateur binaire Draw vs Non-Draw
        """
        self.logger.info("\nüéØ √âTAGE 1: CLASSIFICATEUR DRAWS (D vs H+A)")
        self.logger.info("="*70)
        
        # Test plusieurs algorithmes sp√©cialis√©s pour d√©s√©quilibre
        algorithms_to_test = {
            'LogisticRegression': LogisticRegression(
                random_state=42,
                max_iter=1000,
                class_weight='balanced'
            ),
            'RandomForest': RandomForestClassifier(
                random_state=42,
                class_weight='balanced',
                n_jobs=-1
            ),
            'SVM': SVC(
                random_state=42,
                class_weight='balanced',
                probability=True
            ),
            'XGBoost': xgb.XGBClassifier(
                random_state=42,
                objective='binary:logistic',
                eval_metric='logloss',
                n_jobs=-1
            )
        }
        
        best_model = None
        best_score = 0
        best_algo_name = None
        
        # Tester chaque algorithme
        for algo_name, model in algorithms_to_test.items():
            self.logger.info(f"\nüîç Test {algo_name} pour Draws...")
            
            if algo_name == 'LogisticRegression':
                # Grid search pour LogisticRegression
                param_grid = {
                    'C': [0.01, 0.1, 1.0, 10.0],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear']
                }
                
            elif algo_name == 'RandomForest':
                param_grid = {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [5, 10, 15, None],
                    'min_samples_split': [5, 10],
                    'min_samples_leaf': [2, 4, 8]
                }
                
            elif algo_name == 'SVM':
                param_grid = {
                    'C': [0.1, 1.0, 10.0],
                    'kernel': ['rbf', 'linear'],
                    'gamma': ['scale', 'auto']
                }
                
            elif algo_name == 'XGBoost':
                # Calculer class_weight pour XGBoost manuellement
                class_weights = compute_class_weight('balanced', 
                                                   classes=np.unique(self.y_train_draw),
                                                   y=self.y_train_draw)
                scale_pos_weight = class_weights[1] / class_weights[0]
                
                model.set_params(scale_pos_weight=scale_pos_weight)
                
                param_grid = {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [3, 5, 7],
                    'learning_rate': [0.05, 0.1, 0.2],
                    'subsample': [0.8, 0.9]
                }
            
            # Cross-validation avec stratification (important pour d√©s√©quilibre)
            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
            
            # Grid search
            grid_search = GridSearchCV(
                model, param_grid,
                cv=cv,
                scoring='f1',  # F1-score crucial pour classes d√©s√©quilibr√©es
                n_jobs=-1 if algo_name != 'SVM' else 1,  # SVM plus lent
                verbose=0
            )
            
            # Training
            grid_search.fit(self.X_train_scaled, self.y_train_draw)
            
            # √âvaluation
            best_model_algo = grid_search.best_estimator_
            y_pred = best_model_algo.predict(self.X_test_scaled)
            y_proba = best_model_algo.predict_proba(self.X_test_scaled)[:, 1]
            
            # M√©triques sp√©cialis√©es Draws
            precision, recall, f1, _ = precision_recall_fscore_support(
                self.y_test_draw, y_pred, average='binary'
            )
            
            roc_auc = roc_auc_score(self.y_test_draw, y_proba)
            
            self.logger.info(f"  {algo_name}:")
            self.logger.info(f"    F1-score: {f1:.4f}")
            self.logger.info(f"    Precision: {precision:.4f}")
            self.logger.info(f"    Recall: {recall:.4f}")
            self.logger.info(f"    ROC-AUC: {roc_auc:.4f}")
            self.logger.info(f"    Best params: {grid_search.best_params_}")
            
            # S√©lectionner le meilleur selon F1-score (priorit√© Draws)
            if f1 > best_score:
                best_score = f1
                best_model = best_model_algo
                best_algo_name = algo_name
        
        # Calibration du meilleur mod√®le
        self.logger.info(f"\nüèÜ MEILLEUR POUR DRAWS: {best_algo_name} (F1: {best_score:.4f})")
        
        calibrated_draw = CalibratedClassifierCV(best_model, method='isotonic', cv=3)
        calibrated_draw.fit(self.X_train_scaled, self.y_train_draw)
        
        self.draw_classifier = calibrated_draw
        
        # √âvaluation finale √©tage 1
        y_pred_draw = self.draw_classifier.predict(self.X_test_scaled)
        y_proba_draw = self.draw_classifier.predict_proba(self.X_test_scaled)[:, 1]
        
        precision_final, recall_final, f1_final, _ = precision_recall_fscore_support(
            self.y_test_draw, y_pred_draw, average='binary'
        )
        
        self.logger.info(f"\n‚úÖ √âTAGE 1 FINALIS√â:")
        self.logger.info(f"  Algorithme: {best_algo_name}")
        self.logger.info(f"  F1-score Draws: {f1_final:.4f}")
        self.logger.info(f"  Precision Draws: {precision_final:.4f}")
        self.logger.info(f"  Recall Draws: {recall_final:.4f}")
        
        self.results['draw_classifier'] = {
            'algorithm': best_algo_name,
            'f1_score': float(f1_final),
            'precision': float(precision_final),
            'recall': float(recall_final),
            'roc_auc': float(roc_auc_score(self.y_test_draw, y_proba_draw))
        }
        
        return self.draw_classifier
    
    def train_home_away_classifier(self):
        """
        √âtage 2: Classificateur Home vs Away (pour non-draws)
        """
        self.logger.info(f"\nüè†‚öΩ √âTAGE 2: CLASSIFICATEUR HOME vs AWAY")
        self.logger.info("="*70)
        
        self.logger.info(f"üìä Donn√©es non-draws: {len(self.X_train_ha)} train, {len(self.X_test_ha)} test")
        
        # RandomForest optimis√© pour Home/Away
        param_grid = {
            'n_estimators': [200, 300, 500],
            'max_depth': [10, 15, 20, None],
            'min_samples_split': [5, 10],
            'min_samples_leaf': [2, 4],
            'max_features': ['sqrt', 'log2'],
            'class_weight': ['balanced', None]
        }
        
        rf_ha = RandomForestClassifier(random_state=42, n_jobs=-1)
        
        # Cross-validation 
        cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)
        grid_search = GridSearchCV(
            rf_ha, param_grid,
            cv=cv,
            scoring='accuracy',
            n_jobs=-1,
            verbose=0
        )
        
        # Training
        grid_search.fit(self.X_train_ha_scaled, self.y_train_ha)
        best_model_ha = grid_search.best_estimator_
        
        # Calibration
        calibrated_ha = CalibratedClassifierCV(best_model_ha, method='isotonic', cv=3)
        calibrated_ha.fit(self.X_train_ha_scaled, self.y_train_ha)
        
        self.home_away_classifier = calibrated_ha
        
        # √âvaluation √©tage 2
        y_pred_ha = self.home_away_classifier.predict(self.X_test_ha_scaled)
        y_proba_ha = self.home_away_classifier.predict_proba(self.X_test_ha_scaled)\n        \n        accuracy_ha = accuracy_score(self.y_test_ha, y_pred_ha)
        
        self.logger.info(f"‚úÖ √âTAGE 2 FINALIS√â:")
        self.logger.info(f"  Algorithme: RandomForest")
        self.logger.info(f"  Accuracy Home/Away: {accuracy_ha:.4f} ({accuracy_ha*100:.2f}%)")
        self.logger.info(f"  Best params: {grid_search.best_params_}")
        
        # Feature importance Home/Away
        feature_importance = dict(zip(self.features, best_model_ha.feature_importances_))
        sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        self.logger.info(f"üèÜ Top 5 features Home/Away:")
        for i, (feat, imp) in enumerate(sorted_importance[:5], 1):
            self.logger.info(f"  {i}. {feat}: {imp:.3f}")
        
        self.results['home_away_classifier'] = {
            'algorithm': 'RandomForest',
            'accuracy': float(accuracy_ha),
            'feature_importance': sorted_importance
        }
        
        return self.home_away_classifier
    
    def predict_cascade(self, X):
        """
        Pr√©diction cascade compl√®te
        """
        X_scaled = self.scaler.transform(X)
        
        # √âtage 1: Pr√©dire Draws
        draw_proba = self.draw_classifier.predict_proba(X_scaled)[:, 1]
        draw_pred = self.draw_classifier.predict(X_scaled)
        
        # Initialiser pr√©dictions finales
        final_predictions = np.zeros(len(X), dtype=int)
        final_probabilities = np.zeros((len(X), 3))  # [H, D, A]
        
        # Pour les pr√©dictions Draw (√©tape 1)
        draw_mask = draw_pred == 1
        final_predictions[draw_mask] = 1  # D = 1
        final_probabilities[draw_mask, 1] = draw_proba[draw_mask]
        
        # Pour les non-draws, r√©partir proba entre H et A
        non_draw_mask = draw_pred == 0
        if np.sum(non_draw_mask) > 0:
            X_non_draw = X_scaled[non_draw_mask]
            
            # √âtage 2: Home vs Away pour non-draws
            ha_proba = self.home_away_classifier.predict_proba(X_non_draw)  # [Away, Home]
            ha_pred = self.home_away_classifier.predict(X_non_draw)
            
            # Assigner pr√©dictions
            home_mask_in_non_draw = ha_pred == 1
            away_mask_in_non_draw = ha_pred == 0
            
            # Indices globaux
            global_non_draw_indices = np.where(non_draw_mask)[0]
            
            # Home wins
            global_home_indices = global_non_draw_indices[home_mask_in_non_draw]
            final_predictions[global_home_indices] = 0  # H = 0
            
            # Away wins
            global_away_indices = global_non_draw_indices[away_mask_in_non_draw]
            final_predictions[global_away_indices] = 2  # A = 2
            
            # Probabilit√©s pour non-draws
            # Ajuster selon (1 - proba_draw)
            remaining_proba = 1 - draw_proba[non_draw_mask]
            final_probabilities[non_draw_mask, 0] = ha_proba[:, 1] * remaining_proba  # Home
            final_probabilities[non_draw_mask, 2] = ha_proba[:, 0] * remaining_proba  # Away
            final_probabilities[non_draw_mask, 1] = draw_proba[non_draw_mask]  # Draw
        
        return final_predictions, final_probabilities
    
    def evaluate_cascade(self):
        """
        √âvaluation compl√®te de l'architecture cascade
        """
        self.logger.info(f"\nüéØ √âVALUATION COMPL√àTE CASCADE")
        self.logger.info("="*70)
        
        # Pr√©diction cascade
        y_pred_cascade, y_proba_cascade = self.predict_cascade(self.X_test)
        
        # M√©triques globales
        accuracy_cascade = accuracy_score(self.y_test_global, y_pred_cascade)
        logloss_cascade = log_loss(self.y_test_global, y_proba_cascade)
        
        # Rapport d√©taill√©
        report = classification_report(
            self.y_test_global, 
            y_pred_cascade,
            target_names=['Home', 'Draw', 'Away'],
            output_dict=True
        )
        
        # Focus sur am√©lioration Draws
        draw_f1 = report['Draw']['f1-score']
        draw_precision = report['Draw']['precision']
        draw_recall = report['Draw']['recall']
        
        self.logger.info(f"üèÜ R√âSULTATS CASCADE:")
        self.logger.info(f"  Accuracy globale: {accuracy_cascade:.4f} ({accuracy_cascade*100:.2f}%)")
        self.logger.info(f"  Log-loss: {logloss_cascade:.4f}")
        
        self.logger.info(f"\nüéØ AM√âLIORATION DRAWS:")
        self.logger.info(f"  Draw F1-score: {draw_f1:.4f}")
        self.logger.info(f"  Draw Precision: {draw_precision:.4f}")
        self.logger.info(f"  Draw Recall: {draw_recall:.4f}")
        
        # Comparaison par classe
        self.logger.info(f"\nüìä PERFORMANCE PAR CLASSE:")
        for class_name in ['Home', 'Draw', 'Away']:
            f1 = report[class_name]['f1-score']
            precision = report[class_name]['precision']
            recall = report[class_name]['recall']
            support = report[class_name]['support']
            
            self.logger.info(f"  {class_name:4} -> P: {precision:.3f}, R: {recall:.3f}, F1: {f1:.3f} (n={support})")
        
        # Matrice de confusion
        cm = confusion_matrix(self.y_test_global, y_pred_cascade)
        self.logger.info(f"\nüìä MATRICE CONFUSION:")
        self.logger.info(f"     H    D    A")
        for i, label in enumerate(['H', 'D', 'A']):
            row = ' '.join([f'{cm[i][j]:4d}' for j in range(3)])
            self.logger.info(f"  {label}: {row}")
        
        # √âvaluation du succ√®s
        improvement_threshold = 0.10  # F1 Draw > 0.10 consid√©r√© comme am√©lioration
        success = draw_f1 > improvement_threshold
        
        if success:
            self.logger.info(f"\n‚úÖ SUCC√àS DRAWS: F1={draw_f1:.3f} > {improvement_threshold}")
        else:
            self.logger.info(f"\n‚ö†Ô∏è Am√©lioration limit√©e: F1={draw_f1:.3f} <= {improvement_threshold}")
        
        self.results['cascade_evaluation'] = {
            'global_accuracy': float(accuracy_cascade),
            'global_log_loss': float(logloss_cascade),
            'draw_f1_score': float(draw_f1),
            'draw_precision': float(draw_precision),
            'draw_recall': float(draw_recall),
            'classification_report': report,
            'confusion_matrix': cm.tolist(),
            'success': success,
            'improvement_vs_baseline': float(draw_f1 - 0.04)  # Baseline ~0.04
        }
        
        return success, accuracy_cascade, draw_f1
    
    def save_cascade_model(self, output_path='models/cascade_draws_model.joblib'):
        """
        Sauvegarder le mod√®le cascade complet
        """
        cascade_model = {
            'draw_classifier': self.draw_classifier,
            'home_away_classifier': self.home_away_classifier,
            'scaler': self.scaler,
            'features': self.features,
            'results': self.results
        }
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        joblib.dump(cascade_model, output_path)
        
        self.logger.info(f"‚úÖ Mod√®le cascade sauvegard√©: {output_path}")
        
        return output_path

def main():
    """
    Script principal mod√®le cascade Draws
    """
    print("üéØ MOD√àLE CASCADE SP√âCIALIS√â DRAWS")
    print("="*70)
    print("Objectif: Am√©liorer F1-Draw de 0.04 ‚Üí 0.20+")
    print("Architecture: D vs (H+A) ‚Üí H vs A")
    print("="*70)
    
    cascade = CascadeDrawsModel()
    
    try:
        # 1. Charger donn√©es
        X_train, y_train, X_test, y_test = cascade.load_data()
        
        # 2. Entra√Æner √©tage 1 (Draws)
        draw_classifier = cascade.train_draw_classifier()
        
        # 3. Entra√Æner √©tage 2 (Home/Away)
        ha_classifier = cascade.train_home_away_classifier()
        
        # 4. √âvaluation cascade compl√®te
        success, accuracy, draw_f1 = cascade.evaluate_cascade()
        
        # 5. Sauvegarder
        model_path = cascade.save_cascade_model()
        
        # 6. Rapport final
        timestamp = datetime.now().strftime('%Y_%m_%d_%H%M%S')
        report = {
            'timestamp': timestamp,
            'experiment': 'cascade_draws_model',
            'results': cascade.results,
            'model_path': model_path,
            'success': success,
            'summary': {
                'global_accuracy': cascade.results['cascade_evaluation']['global_accuracy'],
                'draw_f1_improvement': cascade.results['cascade_evaluation']['improvement_vs_baseline'],
                'draw_f1_final': cascade.results['cascade_evaluation']['draw_f1_score']
            }
        }
        
        output_file = f'reports/cascade_draws_report_{timestamp}.json'
        os.makedirs('reports', exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        # Status final
        print("\n" + "="*70)
        print("üéØ CASCADE DRAWS TERMIN√â!")
        print(f"üìä Accuracy globale: {accuracy:.1%}")
        print(f"üéØ Draw F1-score: {draw_f1:.3f}")
        print(f"üìà Am√©lioration: +{(draw_f1-0.04)*100:.1f}pp vs baseline")
        print(f"‚úÖ Succ√®s: {'OUI' if success else 'NON'}")
        print(f"üìÅ Rapport: {output_file}")
        print("="*70)
        
        return 0 if success else 1
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())